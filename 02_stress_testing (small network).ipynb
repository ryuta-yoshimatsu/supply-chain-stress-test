{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f621eb5-3d76-428d-97c5-8f1a4d47b9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4276c642-ba49-4921-95da-dfb23bcf7cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stress-Test Small Networks and Analyze the Results\n",
    "\n",
    "This notebook demonstrates how to run stress tests on a small supply chain network and analyze the results. We will introduce two key concepts—time-to-recover (TTR) and time-to-survive (TTS)—which are essential for understanding the risk exposure of disruption scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649eef1f-e7da-4111-b4fd-95ef1a6db3eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster Configuration\n",
    "This notebook was tested on the following Databricks cluster configuration:\n",
    "- **Databricks Runtime Version:** 16.4 LTS ML (includes Apache Spark 3.5.2, Scala 2.12)\n",
    "- **Single Node** \n",
    "    - Azure: Standard_DS4_v2 (28 GB Memory, 8 Cores)\n",
    "    - AWS: m5d.2xlarge (32 GB Memory, 8 Cores)\n",
    "- **Photon Acceleration:** Disabled (Photon boosts Apache Spark workloads; not all ML workloads will see an improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028faef8-4470-47db-8b11-9ad21e48c7e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install requirements"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d7843b-a847-40e6-81b2-51ccc417d99e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyomo.environ as pyo\n",
    "import scripts.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f850dd8-21e2-4f64-a517-c8481ffd0b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Dataset\n",
    "\n",
    "As in the previous notebook, we will generate a supply chain network along with the corresponding operational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700b3a7b-29df-4630-b57d-66d2a3193290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a synthetic 3-tier supply chain network dataset for optimization\n",
    "# N1: number of product nodes\n",
    "# N2: number of direct supplier nodes\n",
    "# N3: number of sub-supplier nodes\n",
    "dataset = utils.generate_data(N1=5, N2=10, N3=20) # DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94df56c2-573c-4645-bc6f-803ae26eecfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Tier Time-to-Recover (TTR) Model\n",
    "\n",
    "TTR (Time-to-Recover) is a key input to the optimization problem. It represents the time required for a node—or a group of nodes—to return to normal operations after a disruption. TTR is typically obtained from suppliers or estimated through internal assessments.\n",
    "\n",
    "In stress testing, we simulate disruptive scenarios in which one or more nodes fail. A failure can be either partial or complete and lasts for the duration of the TTR. In this solution accelerator, we simulate complete failures for each node. This is modeled by temporarily removing the node and re-optimizing the network based on the new topology. The objective function can be defined flexibly based on your business goals. In this exercise, we define it as the total lost profit across all products, which we aim to minimize. For the detailed formulation and model specification, see the function `utils.build_and_solve_multi_tier_ttr` in the script `scripts/utils.py` or refer to the notebook `04_appendix`.\n",
    "\n",
    "After finding the optimized configuration—through material and inventory reallocation and production adjustments—we obtain the cumulative lost profit incurred during the TTR period. This quantifies the impact of each disruption scenario. We repeat this process for all nodes to compare and assess the relative risk exposure associated with each one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19701fb0-ee23-4ee5-aa91-99781df76a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign a random TTR to each disrupted node; in practice, TTR is a predefined input variable\n",
    "random.seed(777) # DO NOT CHANGE!\n",
    "disrupted_nodes = {node: random.randint(1, 10) for node in dataset['tier2'] + dataset['tier3']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c049adbe-75cf-40e5-a8ad-f2520ad80008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We iterate through all nodes in Tier 2 and Tier 3 and simulate their respective disruption events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cdd7e28-0ae2-4656-8d10-1f54d6e4ec7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lost_profit = []\n",
    "for disrupted_node in disrupted_nodes:\n",
    "    disrupted = [disrupted_node]\n",
    "    df = utils.build_and_solve_multi_tier_ttr(dataset, disrupted, disrupted_nodes[disrupted_node])\n",
    "    lost_profit.append(df)\n",
    "\n",
    "lost_profit = pd.concat(lost_profit, ignore_index=True)\n",
    "display(lost_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69e0a56-7174-425e-9701-b4941e618083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The result above indicates that not every failure leads to a finite lost profit. The magnitude of lost profit is influenced by several factors, including inventory levels, production capacity, supplier diversification, TTR, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84489dc8-0a60-4cdd-9149-7bacf39a789e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Highest Risk Nodes\n",
    "Let's examine the top 5 nodes with the highest lost profit. We'll visualize the network again to help interpret the results more intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0410b8ff-d732-42e4-90e1-4cdf1a4a3fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizes the 3-tier network\n",
    "utils.visualize_network(dataset)\n",
    "\n",
    "highest_risk_nodes = lost_profit.sort_values(by=\"lost_profit\", ascending=False)[0:5]\n",
    "display(highest_risk_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd7e215-1566-464b-824e-2fcbc9d9407b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Having `T2_10` as the node with the highest risk exposure is not surprising, as it is the sole supplier of a specific material type. If this node fails, the production of `T1_1`, `T1_3`, and `T1_5` will halt once existing inventory is depleted.\n",
    "\n",
    "`T2_8` is a more subtle case. At first glance, its high risk is not obvious. However, upon examining the nodes it supplies—`T1_1`, `T1_3`, and `T1_5`—we see that for `T1_1` and `T1_3`, `T2_8` is the only source of a specific material. This means its failure directly halts production at those nodes.\n",
    "\n",
    "For the same reason, `T3_15` is also a high-risk node, with disruption effects that propagate across multiple tiers. `T3_14` is in a similar situation; however, its TTR is 5—significantly shorter than `T3_15`'s TTR of 10. As a result, `T3_14` does not appear on the riskiest node list.\n",
    "\n",
    "Let's take a closer look at a specific failure scenario by inspecting the values of the decision variables identified by the linear solver. In the `utils.build_and_solve_multi_tier_ttr` function, you can set the parameter `return_model=True` to retrieve the model and access the decision variable values. Let's look into the `T2_8` failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba553af-635e-4fe9-b1f2-e6f3edefc695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scenario = \"T2_8\"\n",
    "\n",
    "# Solve the multi-tier TTR model for the given scenario\n",
    "df = utils.build_and_solve_multi_tier_ttr(dataset, [scenario], disrupted_nodes[scenario], True)\n",
    "model = df[\"model\"].values[0]\n",
    "records = []\n",
    "for v in model.component_data_objects(ctype=pyo.Var, active=True):\n",
    "    idx  = v.index()\n",
    "    record  = {\n",
    "        \"var_name\"  : v.parent_component().name,\n",
    "        \"index\"     : idx,\n",
    "        \"value\"     : pyo.value(v),\n",
    "    }\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f85f42b-dd68-4b1b-bddb-76162799065b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demand = pd.DataFrame.from_dict(dataset[\"d\"], orient='index').reset_index()\n",
    "demand = demand.rename(columns={\"index\": \"node\", 0: \"demand\"})\n",
    "demand[\"demand\"] *= disrupted_nodes[scenario]\n",
    "\n",
    "prod = pd.DataFrame([record for record in records if record[\"var_name\"] == \"u\"])\n",
    "prod = prod.rename(columns={\"index\": \"node\", \"value\": \"production\"})\n",
    "prod = prod.drop(\"var_name\", axis=1).sort_values(by=\"node\").reset_index(drop=True)\n",
    "\n",
    "stock = pd.DataFrame.from_dict(dataset[\"s\"], orient='index').reset_index()\n",
    "stock = stock.rename(columns={\"index\": \"node\", 0: \"stock\"})\n",
    "\n",
    "lost = pd.DataFrame([record for record in records if record[\"var_name\"] == \"l\"])\n",
    "lost = lost.drop(\"var_name\", axis=1).rename(columns={\"index\": \"node\", \"value\": \"lost_demand\"})\n",
    "\n",
    "merged = demand.merge(prod, on=\"node\", how=\"right\")\n",
    "merged = merged.merge(stock, on=\"node\")\n",
    "merged = merged.merge(lost, on=\"node\", how=\"left\")\n",
    "\n",
    "display(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25b9a66-2405-449c-9008-eec163ee05bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The first thing to notice in the output above is that the production volume of `T2_8` is zero due to the simulated failure (`production`). Next, we observe losses at `T1_1` and `T1_3` (`lost_demand`). In the case of `T1_3`, there is no production, so once the existing stock is depleted, the remaining demand directly becomes lost demand. For `T1_1`, it appears that all of `T2_8`'s inventory (1,569 units) was routed to this node, allowing some production to continue. The solver prioritized `T1_1` because it has a higher profit margin than `T1_3` (as shown in the `01_operational_data` notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13801c64-7de8-4ab8-8534-084eb87731a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Tier Time-to-Survive (TTS) Model\n",
    "\n",
    "Time-to-Survive (TTS) provides a complementary perspective on the risk associated with node failures. Unlike TTR, TTS is not an input but an output—a decision variable. When a disruption affects a node or group of nodes, TTS represents the amount of time the reconfigured network can continue to meet demand without incurring any loss. The risk becomes more critical when TTR exceeds TTS by a significant margin.\n",
    "\n",
    "The TTS model closely resembles the TTR model, with minor modifications to the objective function and constraints. For more details, see the function `utils.build_and_solve_multi_tier_tts` in the script `scripts/utils.py` or refer to the `04_appendix` notebook.\n",
    "\n",
    "Let's again iterate through all nodes in Tier 2 and Tier 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e03d2e-737c-4665-b961-ca450e323ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tts = []\n",
    "for disrupted_node in disrupted_nodes:\n",
    "    disrupted = [disrupted_node]\n",
    "    df = utils.build_and_solve_multi_tier_tts(dataset, disrupted)\n",
    "    df[\"ttr\"] = disrupted_nodes[disrupted_node]\n",
    "    tts.append(df)\n",
    "\n",
    "# Concatenate all dataframes in the list into a single dataframe\n",
    "tts = pd.concat(tts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac7707d-aadb-4476-9825-b7acd7a4f85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, let's show where TTS is shorter than TTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334473ad-579d-4da7-9f01-21eb643a4c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show where tts is shorter than ttr\n",
    "display(tts[tts[\"tts\"] < tts[\"ttr\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5378407f-f414-4fb8-8f53-2cd913605414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The result above highlights all (sub)supplier sites where the time-to-recover (TTR) exceeds the time-to-survive (TTS). To enhance network resiliency, these nodes can be reassessed. Potential actions include reducing TTR through supplier renegotiations, increasing TTS by building inventory buffers, or diversifying the sourcing strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824af72c-f5ea-4c25-a8ad-db15aff1a036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this notebook, we demonstrated how to run stress tests on a small supply chain network and analyze the results. We introduced two key concepts—time-to-recover (TTR) and time-to-survive (TTS)—and applied them in our analysis.\n",
    "\n",
    "In the next notebook, `03_stress_testing (large network)`, we will run similar stress tests on a much larger network consisting of thousands of nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790bfb22-8df6-4572-94ce-43b2a5c4e7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License [https://databricks.com/db-license-source].  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| pyomo | An object-oriented algebraic modeling language in Python for structured optimization problems | BSD | https://pypi.org/project/pyomo/\n",
    "| highspy | Linear optimization solver (HiGHS) | MIT | https://pypi.org/project/highspy/\n",
    "| ray | Framework for scaling AI/Python applications | Apache 2.0 | https://github.com/ray-project/ray"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_stress_testing (small network)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
