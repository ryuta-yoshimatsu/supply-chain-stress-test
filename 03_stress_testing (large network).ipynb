{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549eac82-dae9-426c-abe0-70b7654b337d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7160f2df-88c3-4e1b-8deb-6d510d7cc62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stress-Test Large Networks and Analyze the Results\n",
    "\n",
    "This notebook demonstrates how to perform stress testing on a large supply chain network. While the previous notebooks focused on a small network (35 nodes), modern supply chains often consist of tens of thousands of suppliers and sub-suppliers. To run comprehensive stress tests on such large-scale networks, a scalable setup is essential. We leverage distributed computation using [Ray on Databricks](https://docs.databricks.com/aws/en/machine-learning/ray/) to achieve this. This notebook covers network generation (1,700 nodes), Ray cluster setup, distributed optimization, and result analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad35e69-d0ce-4a79-9a7a-00f066789083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster Configuration\n",
    "This notebook was tested on the following Databricks cluster configuration:\n",
    "- **Databricks Runtime Version:** 16.4 LTS ML (includes Apache Spark 3.5.2, Scala 2.12)\n",
    "- **Driver Type** \n",
    "    - Azure: Standard_DS4_v2 (28 GB Memory, 8 Cores)\n",
    "    - AWS: m5d.2xlarge (32 GB Memory, 8 Cores)\n",
    "- **Worker Type** \n",
    "    - Azure: Standard_E4d_v4 (32 GB Memory, 4 Cores)\n",
    "    - AWS: rd-fleet.xlarge (32 GB Memory, 4 Cores)\n",
    "- **Number of Workers:** 4\n",
    "- **Photon Acceleration:** Disabled (Photon boosts Apache Spark workloads; not all ML workloads will see an improvement)\n",
    "> **Note:** Performance may vary depending on the cluster size, node types, and workload characteristics. For large-scale distributed computation, ensure sufficient resources are allocated to avoid bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7808ef-667b-4527-a5a4-5a93448de5e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install requirements"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4a5969-c2ab-47df-bc9d-a8ed9c68fa84",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import modules"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.common.timing import TicTocTimer\n",
    "import scripts.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de68fb4-becd-42d2-9a92-e4c621d09c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will write the results of our optimization to Delta tables. Update the `catalog` and `schema` names below to specify where you want the results to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bd36a9-5bdc-4faa-bae8-4bbe4a6b67c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"supply_chain_stress_test\" # Change here\n",
    "schema = \"results\"                   # Change here\n",
    "\n",
    "# Make sure that the catalog and the schema exist\n",
    "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\") \n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f89534-01b8-4684-afe3-ae873582ddfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Data\n",
    "We generate a synthetic 3-tier supply chain network dataset for optimization. We also assign random time-to-recovery (ttr) values to each disrupted node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367a0f02-bdfe-46c9-a5e7-d86e7d8c08cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a synthetic 3-tier network dataset for optimization \n",
    "dataset = utils.generate_data(N1=200, N2=500, N3=1000) # DO NOT CHANGE!\n",
    "\n",
    "# Assign a random ttr (time-to-recovery) to each disrupted node\n",
    "random.seed(777) # DO NOT CHANGE!\n",
    "disrupted_nodes = {node: random.randint(1, 10) for node in dataset['tier2'] + dataset['tier3']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef6ed83-cef7-4f4b-9434-e1571abfbbc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Retrieve Databricks Cluster Information\n",
    "This function retrieves the minimum and maximum number of worker nodes from the Databricks cluster context using the REST API. This information is used to configure the Ray cluster for distributed computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b202acc-07ff-4a2c-aa18-5c9a2f78b4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks-only: get cluster context and min/max nodes\n",
    "def get_min_max_nodes():\n",
    "    try:\n",
    "        import requests\n",
    "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        host_name = ctx.browserHostName().get()\n",
    "        host_token = ctx.apiToken().get()\n",
    "        cluster_id = ctx.clusterId().get()\n",
    "        response = requests.get(\n",
    "            f'https://{host_name}/api/2.1/clusters/get?cluster_id={cluster_id}',\n",
    "            headers={'Authorization': f'Bearer {host_token}'}\n",
    "        ).json()\n",
    "        if \"autoscale\" in response:\n",
    "            return response['autoscale'][\"min_workers\"], response['autoscale'][\"max_workers\"]\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Warning: Could not fetch min/max nodes from Databricks context: {e}\")\n",
    "\n",
    "    return 1, response['num_workers']  # fallback for local/testing\n",
    "\n",
    "min_node, max_node = get_min_max_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149ce901-95e2-4863-b02a-629762b966f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ray Cluster Initialization\n",
    "Ray is a distributed execution framework that enables scalable parallel computation. Here, we initialize a Ray cluster on Databricks using the `setup_ray_cluster` utility. The number of worker nodes and CPU cores per node are set based on the Databricks cluster configuration. Environment variables for Databricks authentication are also set for Ray workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d64e1bf-3703-4be6-9599-2f66b8fc263c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# Cluster cleanup: shut down any existing Ray cluster and Ray context to ensure a clean start\n",
    "restart = True\n",
    "if restart is True:\n",
    "    try:\n",
    "        shutdown_ray_cluster()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        ray.shutdown()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Set configs based on your cluster size\n",
    "num_cpu_cores_per_worker = 4 # total cpu to use in each worker node \n",
    "num_cpus_head_node = 4 # Cores to use in driver node (total_cores - 4)\n",
    "\n",
    "# Set databricks credentials as env vars for Ray workers\n",
    "try:\n",
    "    from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "    mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "    os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not set Databricks env vars: {e}\")\n",
    "\n",
    "# Start the Ray cluster with the specified configuration\n",
    "ray_conf = setup_ray_cluster(\n",
    "    min_worker_nodes=min_node,\n",
    "    max_worker_nodes=max_node,\n",
    "    num_cpus_head_node=num_cpus_head_node,\n",
    "    num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "    num_gpus_head_node=0,\n",
    "    num_gpus_worker_node=0\n",
    ")\n",
    "os.environ['RAY_ADDRESS'] = ray_conf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77188cda-c5b7-414d-b6ad-57188b7033a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Data for Distributed Computation\n",
    "Here, we convert the disrupted nodes dictionary we defined above to a pandas DataFrame, then to a Ray Dataset for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0918c8c-e3e5-4e2e-9744-6021070a525f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(disrupted_nodes, orient='index', columns=['ttr']).reset_index(names='node')\n",
    "df = ray.data.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b81d01-aa96-412f-b28a-5b0ea0e60c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Tier TTR Model\n",
    "\n",
    "### Define the Solver Class\n",
    "The `TTRSolver` class encapsulates the logic for running the `utils.build_and_solve_multi_tier_ttr` function for each disrupted scenario. It is designed to be used with Ray's [distributed map](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8f3d6d-7b5d-4a8f-b59c-3aea8981d39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TTRSolver:\n",
    "    \"\"\"\n",
    "    Callable class to run the Pyomo model for a single disrupted scenario.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data=dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __call__(self, row):\n",
    "        \"\"\"Run the Pyomo model for a single disrupted scenario.\"\"\"\n",
    "        disrupted = [row['node']]\n",
    "        # Call the utility function to build and solve the optimization model\n",
    "        solver = utils.build_and_solve_multi_tier_ttr(self.data, disrupted, row['ttr'])\n",
    "        row['termination_condition'] = str(solver.iloc[0]['termination_condition'])\n",
    "        row['lost_profit'] = solver.iloc[0]['lost_profit']\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d7de29-e1ae-4e04-a650-611b1472a364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test the Solver on a Single Row\n",
    "This cell tests the `TTRSolver` class on a single row to ensure correctness before distributed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f705e8a0-8854-459f-8192-3e027e61c052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TTRSolver()(df.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a3bda8-0ee8-4726-b7ba-5697d5ac9bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Distributed Computation with Ray Data API\n",
    "The following cell demonstrates distributed computation using [Ray's Data API](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html):\n",
    "- The Ray Dataset is repartitioned into 300 partitions to increase parallelism and optimize resource utilization across the cluster.\n",
    "- The `map` function applies the `TTRSolver` class to each partition in parallel, with each task using 1 CPU and a concurrency window of (4, 20) you can adjust the concurreny based on your cluster setup.\n",
    "- The results are collected as a pandas DataFrame for further analysis.\n",
    "\n",
    "**The following cell will run in about a minute. On a single-node cluster without distributed computation, the same calculation would take approximately an hour to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc88c549-8ea3-4c79-a8db-ba5921378005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ttr = df.repartition(300).map(TTRSolver,\n",
    "       num_cpus=1,\n",
    "       concurrency=(4,20))\n",
    "pandas_df_ttr = df_ttr.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58326358-2ecc-49a5-a6dc-19157296f6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Highest Risk Nodes\n",
    "Let's examine the top 10 nodes with the highest lost profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524a6fe-fbc9-44ab-ab6d-cc13e5b069c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "highest_risk_nodes = pandas_df_ttr.sort_values(by=\"lost_profit\", ascending=False)[0:10]\n",
    "display(highest_risk_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54245683-b9e7-47c1-a4a9-96b64e3ec896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Total Spend vs. Lost Profit\n",
    "\n",
    "Let's imagine we have a global budget for risk mitigation in our supply chain, and each node receives some portion of that budget. The purpose of this analysis is to identify which nodes are over- or under-invested based on the risk exposure we previously computed. For simplicity, we randomly assign the total spend on risk mitigation measures for each node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b046e3-7afe-4f70-9850-a33e97b0270c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # DO NOT CHANGE!\n",
    "pandas_df_ttr[\"total_spend\"] = np.abs(np.random.normal(loc=0, scale=50, size=len(pandas_df_ttr))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3932f27e-8afc-4be3-a5ad-bc868adcba59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.scatter(pandas_df_ttr[\"lost_profit\"], pandas_df_ttr[\"total_spend\"])\n",
    "plt.xlabel(\"Lost Profit\")\n",
    "plt.ylabel(\"Total Spend\")\n",
    "plt.title(\"Total Spend vs Lost Profit\")\n",
    "\n",
    "rect_1 = patches.Rectangle((1900, -5), 3100, 110, linewidth=2, edgecolor='red', facecolor='none')\n",
    "rect_2 = patches.Rectangle((-50, 100), 1000, 100, linewidth=2, edgecolor='red', facecolor='none')\n",
    "plt.gca().add_patch(rect_1)\n",
    "plt.gca().add_patch(rect_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aba30e4-3338-4f27-905c-8a582dc950ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The scatter plot above shows total spend on supplier sites for risk mitigation on the vertical axis and lost profit on the horizontal axis. This visualization helps quickly identify areas where risk mitigation investment is undersized relative to the potential impact of a node failure (right box), as well as areas where investment may be oversized relative to the risk (left box and potentially all nodes with zero lost profit). Both regions highlight opportunities to reassess and optimize the investment strategy—either to strengthen network resiliency or reduce unnecessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b845bec7-28ce-436a-b0f1-d71956a8865d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Tier TTS Model\n",
    "\n",
    "### Define the Solver Class\n",
    "The `TTSSolver` class encapsulates the logic for running the `utils.build_and_solve_multi_tier_tts` function for each disruption scenario. It will reuse the same Ray cluster defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a634a76-f5bd-4928-bf25-1b52d757e16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TTSSolver:\n",
    "    \"\"\"\n",
    "    Callable class to run the Pyomo model for a single disrupted scenario.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data=dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __call__(self, row):\n",
    "        \"\"\"Run the Pyomo model for a single disrupted scenario.\"\"\"\n",
    "        disrupted = [row['node']]\n",
    "        # Call the utility function to build and solve the optimization model\n",
    "        solver = utils.build_and_solve_multi_tier_tts(self.data, disrupted)\n",
    "        row['termination_condition'] = str(solver.iloc[0]['termination_condition'])\n",
    "        row['tts'] = solver.iloc[0]['tts']\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a84662-0817-4e12-8781-215bbc152f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test the Solver on a Single Row\n",
    "This cell tests the `TTSSolver` class on a single row to ensure correctness before distributed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df313267-2dbe-4d97-b098-db4c0463527e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TTSSolver()(df.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106c66d6-69c3-4033-8e64-3449a75a0df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run the Solver at Scale\n",
    "\n",
    "Let's solve the TTS model at scale. The following cell will run for approximately 30 minutes using the cluster configuration mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae56eeb-1336-4bde-9484-bc6b6af72d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tts = df.repartition(300).map(TTSSolver,\n",
    "                                 num_cpus=1,\n",
    "                                 concurrency=(4,20))\n",
    "pandas_df_tts = df_tts.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2d5eb4-2f30-46de-8bdf-3bc11446a5e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e272a136-db6a-403f-a244-2ce5acc0e498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pandas_df_tts['delta'] = pandas_df_tts['ttr'] - pandas_df_tts['tts']\n",
    "ax = pandas_df_tts.hist(column='delta', bins=20, grid=False, edgecolor='black', figsize=(10, 6))\n",
    "plt.title('Histogram of TTR - TTS')\n",
    "plt.xlabel('TTR - TTS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "display(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d08e13-63d6-4df2-bf93-f29132f2338d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that TTS represents the maximum amount of time the network can operate without performance loss when a specific node is disrupted. It becomes particularly important when a node’s TTR exceeds its TTS.\n",
    "\n",
    "Refer to the histogram above, which shows the distribution of differences between TTR and TTS for each node. Nodes with a negative TTR − TTS are generally not a concern—assuming the provided TTR values are accurate. However, nodes with a positive TTR − TTS may incur financial loss, especially those with a large gap.\n",
    "\n",
    "To enhance network resiliency, companies can engage in discussions with their suppliers to reduce TTR, increase TTS or explore alternative sourcing and diversification strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d9fc42-6bd2-4e86-af87-4e8b3fd273e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Shutdown Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2a2209-aed5-4fc1-8195-4b0ea9db6150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutdown_ray_cluster()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    ray.shutdown()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e86bb2-2c8f-442c-b168-66b3797b7abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc40e62-8e93-4e9e-8e21-6d07ac35d72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks-only: save to Delta table\n",
    "try:\n",
    "    spark.createDataFrame(pandas_df_ttr).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.stress_test_result_ttr\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save to Delta table: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.createDataFrame(pandas_df_tts).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.stress_test_result_tts\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save to Delta table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51d55b4-70af-42c7-9db2-811991f66e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this notebook, we explored how to perform stress testing on a large supply chain network. We leveraged Ray on Databricks to distribute the simulation of thousands of disruption scenarios. We then analyzed the distribution of risk exposures across these scenarios and identified nodes that may require additional investment, as well as those that may have been previously over-invested.\n",
    "\n",
    "This concludes the main part of the solution accelerator. The next notebook, `04_appendix`, is optional. It dives into the mathematical formulation of the optimization problem and discusses key assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "616cfb2e-f393-4aa4-a3af-f949747e64b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License [https://databricks.com/db-license-source].  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| pyomo | An object-oriented algebraic modeling language in Python for structured optimization problems | BSD | https://pypi.org/project/pyomo/\n",
    "| highspy | Linear optimization solver (HiGHS) | MIT | https://pypi.org/project/highspy/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_stress_testing (large network)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
